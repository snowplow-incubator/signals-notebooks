{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKNmeipisHA5"
      },
      "source": [
        "# Score prospects in real time using Snowplow Signals and machine learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXB3RUid-PU2"
      },
      "source": [
        "Welcome to the [Snowplow Signals](https://docs.snowplow.io/docs/signals/) real-time prospect scoring [tutorial](https://docs.snowplow.io/tutorials/signals-ml-prospect-scoring/intro).\n",
        "\n",
        "This notebook is intended to be used as part of the tutorial. It's hosted in Google Colab so you won't need to configure anything locally.\n",
        "\n",
        "Check out the tutorial pages to follow along."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPy8d6ti-PU3"
      },
      "source": [
        "## Set up the notebook\n",
        "\n",
        "This notebook uses Signals Sandbox connection. If you use Signals in the Console uncomment relevant lines.\n",
        "\n",
        "Add your credentials to the Colab notebook secrets. Or just replace these variable values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHIGalfqXnyd"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Signals Sandbox\n",
        "SP_SANDBOX_URL = userdata.get('SP_SANDBOX_URL')     # https://{{123abc}}.svc.snplow.net\n",
        "SP_SANDBOX_TOKEN = userdata.get('SP_SANDBOX_TOKEN') # 12345678-0000-1111-2222-123456789012\n",
        "\n",
        "# Uncomment for Signals in the Console\n",
        "# SP_API_URL = userdata.get('SP_API_URL')       # Signals API URL\n",
        "# SP_API_KEY = userdata.get('SP_API_KEY')       # Signals API key\n",
        "# SP_API_KEY_ID = userdata.get('SP_API_KEY_ID') # Signals API key ID\n",
        "# SP_ORG_ID = userdata.get('SP_ORG_ID')         # Snowplow org ID"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install notebook dependencies"
      ],
      "metadata": {
        "id": "Z8aJQuxOgE2v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtTWUlSw-PU5"
      },
      "outputs": [],
      "source": [
        "%pip install snowplow-signals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KZmrOjd-PU6"
      },
      "source": [
        "### Connect to Signals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifn7MLJC-PU6"
      },
      "outputs": [],
      "source": [
        "from snowplow_signals import Signals, SignalsSandbox, Event\n",
        "from datetime import timedelta\n",
        "\n",
        "sp_signals = SignalsSandbox(\n",
        "    api_url=SP_SANDBOX_URL,\n",
        "    sandbox_token=SP_SANDBOX_TOKEN,\n",
        ")\n",
        "\n",
        "# Uncomment if you use Signals in the Console\n",
        "# sp_signals = Signals(\n",
        "#     api_url=SP_API_URL,\n",
        "#     api_key=SP_API_KEY,\n",
        "#     api_key_id=SP_API_KEY_ID,\n",
        "#     org_id=SP_ORG_ID\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCbcJCbP-PU7"
      },
      "source": [
        "### Define attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQegUKdw-PU7"
      },
      "outputs": [],
      "source": [
        "from snowplow_signals import Attribute, Criteria, Criterion, AtomicProperty, EntityProperty\n",
        "\n",
        "# Define an event\n",
        "sp_page_view = Event(\n",
        "    vendor=\"com.snowplowanalytics.snowplow\",\n",
        "    name=\"page_view\",\n",
        "    version=\"1-0-0\"\n",
        ")\n",
        "\n",
        "# Define attributes\n",
        "num_page_views = Attribute(\n",
        "    name=\"num_page_views\",\n",
        "    type=\"int32\",\n",
        "    events=[sp_page_view],\n",
        "    aggregation=\"counter\"\n",
        ")\n",
        "\n",
        "num_pricing_views = Attribute(\n",
        "    name=\"num_pricing_views\",\n",
        "    type=\"int32\",\n",
        "    events=[sp_page_view],\n",
        "    aggregation=\"counter\",\n",
        "    criteria=Criteria(\n",
        "        all=[\n",
        "            Criterion.like(\n",
        "                AtomicProperty(name=\"page_url\"),\n",
        "                \"%pricing%\"\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "num_customers_views = Attribute(\n",
        "    name=\"num_customers_views\",\n",
        "    type=\"int32\",\n",
        "    events=[sp_page_view],\n",
        "    aggregation=\"counter\",\n",
        "    criteria=Criteria(\n",
        "        all=[\n",
        "            Criterion.like(\n",
        "                AtomicProperty(name=\"page_url\"),\n",
        "                \"%customers%\"\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFWJfQuW-PU7"
      },
      "source": [
        "### Define an attribute group and a service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KU3vCeo0-PU7"
      },
      "outputs": [],
      "source": [
        "from snowplow_signals import StreamAttributeGroup, domain_userid, Service\n",
        "\n",
        "user_attributes_group = StreamAttributeGroup(\n",
        "    name='prospect_scoring_tutorial',\n",
        "    owner='your_email@example.com',\n",
        "    version=1,\n",
        "    attribute_key=domain_userid,\n",
        "    attributes=[\n",
        "        num_page_views,\n",
        "        num_pricing_views,\n",
        "        num_customers_views,\n",
        "    ],\n",
        ")\n",
        "\n",
        "prospect_scoring_tutorial_service = Service(\n",
        "    name='prospect_scoring_tutorial_service',\n",
        "    owner='your_email@example.com',\n",
        "    attribute_groups=[user_attributes_group],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjE7xmv6-PU8"
      },
      "source": [
        "**Note:** if you're using Signals in the Console, you can test attributes on the last 1 hour of data like this:\n",
        "\n",
        "```python\n",
        "sp_signals.test(attribute_group=user_attributes_group, app_ids=[\"website\"])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "860gN7Rq-PU8"
      },
      "source": [
        "### Deploy configuration to Signals\n",
        "\n",
        "Running this cell will connect to your Signals instance. Signals will start calculating attributes to populate your Profiles Store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBSX-Atz-PU8"
      },
      "outputs": [],
      "source": [
        "published = sp_signals.publish([user_attributes_group, prospect_scoring_tutorial_service])\n",
        "\n",
        "# This should print \"2 objects published\"\n",
        "print(f\"{len(published)} objects published\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** you can't publish the same version of the attribute group or service a couple times. You will need to increase the version number or unpublish+delete the existing versions. You can use the code below during the tutorial to reset the attribute group and service after publishing them.\n",
        "\n",
        "\n",
        "```python\n",
        "sp_signals.unpublish([prospect_scoring_tutorial_service])\n",
        "sp_signals.unpublish([user_attributes_group])\n",
        "sp_signals.delete([prospect_scoring_tutorial_service])\n",
        "sp_signals.delete([user_attributes_group])\n",
        "```"
      ],
      "metadata": {
        "id": "g4CPQQEmgREF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAp3gI7X-PU8"
      },
      "source": [
        "### Look at your attributes\n",
        "\n",
        "Use the Snowplow Inspector browser plugin to find your current `domain_userid`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUhdNgm4-PU9"
      },
      "outputs": [],
      "source": [
        "sp_signals_result = sp_signals.get_service_attributes(\n",
        "    name=\"prospect_scoring_tutorial_service\",\n",
        "    attribute_key=\"domain_userid\",\n",
        "    identifier=\"00000000-1111-2222-3333-444455556666\", # UPDATE THIS\n",
        ")\n",
        "sp_signals_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFa3xi5g-PU9"
      },
      "source": [
        "## Train the ML prospect scoring model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_mvQOnZ-PU-"
      },
      "source": [
        "### Prepare historical dataset\n",
        "\n",
        "The code in this cell uses a sample of the Snowplow atomic table in a CSV format. Update this cell to connect to your warehouse and train on your own data. Duckdb SQL syntax is easily translatable to the common warehouses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOCRL94jaKoD"
      },
      "outputs": [],
      "source": [
        "csv_filename = 'https://github.com/snowplow-incubator/signals-notebooks/raw/refs/heads/main/web/sample_events.csv.gz'\n",
        "\n",
        "query = f\"\"\"\n",
        "\twith\n",
        "\t-- Change to your events table\n",
        "\tevents as (\n",
        "\t    select * from read_csv_auto('{csv_filename}')\n",
        "\t    where app_id = 'website' -- filter for your app_ids\n",
        "\t        and domain_userid is not null -- filter to traffic with domain_userids\n",
        "\t        and event_name in ('page_view', 'submit_form') -- filter the events you need\n",
        "\t        -- and derived_tstamp >= dateadd('day', -90, current_date) -- filter by time\n",
        "\t        -- filter out bots, new/returning users, etc\n",
        "\t),\n",
        "\t-- Filter out post-conversion events\n",
        "\tpost_conv as (\n",
        "\t\tselect\n",
        "\t\t\tdomain_userid,\n",
        "\t\t\tmin(derived_tstamp) as first_conv\n",
        "\t\tfrom events\n",
        "\t\twhere event_name = 'submit_form'\n",
        "\t\tgroup by 1\n",
        "\t),\n",
        "\t-- Prepare the target column\n",
        "\ttargets_as_of_event as (\n",
        "\t    -- Target: will this person 'submit_form' in the next 1 hour?\n",
        "\t    select\n",
        "\t        -- identifiers\n",
        "\t        er.event_id,\n",
        "\t        -- target\n",
        "\t        count_if(ef.event_name = 'submit_form') > 0 as target_had_submit_form_next1h,\n",
        "\t    from events er\n",
        "\t    left join events ef on er.domain_userid = ef.domain_userid\n",
        "\t        and er.derived_tstamp < ef.derived_tstamp -- only future events\n",
        "\t        and datediff('second', er.derived_tstamp, ef.derived_tstamp) <= 60 * 60 -- only the next 1h of events\n",
        "\t    group by er.event_id\n",
        "\t),\n",
        "\t-- Prepare training features and dataset\n",
        "\tfinal_training as (\n",
        "\t    select\n",
        "\t\t\ter.domain_userid,\n",
        "\t        er.domain_sessionid,\n",
        "\t        er.derived_tstamp,\n",
        "\t        er.event_name,\n",
        "\t        coalesce(count_if(eh.event_name = 'page_view'), 0) as num_page_views,\n",
        "\t        coalesce(count_if(eh.event_name = 'page_view' and eh.page_url like '%pricing%'), 0) as num_pricing_views,\n",
        "\t        coalesce(count_if(eh.event_name = 'page_view' and eh.page_url like '%customers%'), 0) as num_customers_views,\n",
        "\t        coalesce(t.target_had_submit_form_next1h, false) as target_had_submit_form_next1h,\n",
        "\t        p.domain_userid is not null as remove_as_post_conv,\n",
        "\t    from events er\n",
        "\t    left join events eh on er.domain_userid = eh.domain_userid\n",
        "\t        and eh.derived_tstamp < er.derived_tstamp\n",
        "\t    left join targets_as_of_event t on er.event_id = t.event_id\n",
        "\t    left join post_conv p on er.domain_userid = p.domain_userid\n",
        "\t    \tand er.derived_tstamp >= p.first_conv\n",
        "\t    group by all\n",
        "      having not p.domain_userid is not null -- remove post-conversion events\n",
        "\t)\n",
        "\tselect * from final_training\n",
        "\"\"\"\n",
        "\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "\n",
        "conn = duckdb.connect(database=':memory:')\n",
        "db_df = conn.sql(query).df()\n",
        "conn.close()\n",
        "\n",
        "db_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqizRUYeb0HX"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx89cA4C1_he"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# preprocessing\n",
        "x_columns = [\n",
        "    'num_page_views',\n",
        "    'num_pricing_views',\n",
        "    'num_customers_views',\n",
        "]\n",
        "y_column = 'target_had_submit_form_next1h'\n",
        "\n",
        "seed = 42\n",
        "model = LogisticRegression(random_state=seed, class_weight='balanced')\n",
        "\n",
        "# Split, Train, and Evaluate\n",
        "X = db_df[x_columns]\n",
        "y = db_df[y_column]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Export\n",
        "joblib.dump(model, \"model.joblib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsHjDn74Zz9Z"
      },
      "source": [
        "### Evaluate model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOcn14EAZxTW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss, ConfusionMatrixDisplay, roc_curve, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Quick Model Metric Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "print(\"ROC AUC: \", roc_auc_score(y_test, y_prob))\n",
        "print(\"Log Loss:\", log_loss(y_test, y_prob))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Visualize Confusion Matrix and ROC Curve\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, cmap='Blues', ax=axes[0])\n",
        "axes[0].set_title(\"Confusion Matrix\")\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "axes[1].plot(fpr, tpr, label=f'ROC AUC: {roc_auc_score(y_test, y_prob):.2f}')\n",
        "axes[1].plot([0, 1], [0, 1], 'k--')\n",
        "axes[1].set_xlabel(\"False Positive Rate\")\n",
        "axes[1].set_ylabel(\"True Positive Rate\")\n",
        "axes[1].legend()\n",
        "axes[1].set_title(\"ROC Curve\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPRaWQrubXGf"
      },
      "source": [
        "## Create ML API endpoint and Score Prospects"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create and serve an API endpoint\n",
        "\n",
        "For simplicity we use free TryCloudflare tunnels to expose this Colab notebook behind an HTTPS endpoint.\n",
        "\n",
        "When this cell runs, you should see your URL like `https://abc-123.trycloudflare.com` in the output.\n"
      ],
      "metadata": {
        "id": "PWaf2-bN7eE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install flask-cloudflared"
      ],
      "metadata": {
        "id": "Z86islTfjpoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request\n",
        "from flask_cloudflared import run_with_cloudflared\n",
        "\n",
        "# Define and launch Colab API Proxy\n",
        "app = Flask(__name__)\n",
        "run_with_cloudflared(app) # Open Cloudflared demo tunnel\n",
        "\n",
        "# Load model\n",
        "model = joblib.load(\"model.joblib\")\n",
        "\n",
        "# Process the Signals data for individual domain_userids\n",
        "def get_duid_values(duid: str):\n",
        "    # Retrieve attributes from Signals\n",
        "    response = sp_signals.get_service_attributes(\n",
        "        name=\"prospect_scoring_tutorial_service\",\n",
        "        attribute_key=\"domain_userid\",\n",
        "        identifier=duid,\n",
        "    )\n",
        "    signals_df = pd.DataFrame([response])\n",
        "    # Prepare ML dataframe\n",
        "    ml_df = signals_df.fillna(0).reindex(columns=x_columns, fill_value=0)\n",
        "    return (signals_df, ml_df)\n",
        "\n",
        "# Score the data against the model\n",
        "def get_predictions(df):\n",
        "    return float(model.predict_proba(df)[:, 1][0])\n",
        "\n",
        "@app.route(\"/predict\", methods=['POST'])\n",
        "def predict():\n",
        "    input_dict = request.get_json() # Parse JSON input\n",
        "\n",
        "    # Get Signals data and prepare dataframe for scoring\n",
        "    signals_df, ml_df = get_duid_values(input_dict['domain_userid'])\n",
        "\n",
        "    # Score dataframe using the trained model\n",
        "    prediction = get_predictions(ml_df)\n",
        "\n",
        "    # Return the result\n",
        "    print(f\"P: {round(prediction, 4)} - {input_dict}\")\n",
        "    return {\n",
        "        \"signals\": signals_df.to_dict(orient='records')[0],\n",
        "        \"scoring_attributes\": ml_df.to_dict(orient='records')[0],\n",
        "        \"score\": prediction\n",
        "    }\n",
        "\n",
        "app.run()"
      ],
      "metadata": {
        "id": "IqLGvPSQv5P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxnniG51f56q"
      },
      "source": [
        "### Test using cURL\n",
        "\n",
        "Verify via cURL, using the ngrok URL and your current `domain_userid` on your website:\n",
        "\n",
        "```bash\n",
        "curl -X POST \"https://<YOUR_CLOUDFLARED_URL>.trycloudflare.com/predict\" \\\n",
        "-H \"Content-Type: application/json\" \\\n",
        "-d '{\"domain_userid\": \"UPDATE THIS\"}'\n",
        "```\n",
        "\n",
        "### See scores in the browser\n",
        "\n",
        "Verify via JS in the browser console:\n",
        "\n",
        "```js\n",
        "let api_url = \"https://<UPDATE_THIS>.trycloudflare.com/predict\"; // UPDATE THIS\n",
        "let tracker_name = \"sp\"; // MAYBE UPDATE THIS\n",
        "\n",
        "// Calls the API every 10s from the front-end\n",
        "setInterval(function () {\n",
        "    // assuming the Snowplow tracker is available at 'window.snowplow(...)'\n",
        "    window.snowplow(function () {\n",
        "        // Gets domain_userid from the tracker instance\n",
        "        var sp = this[tracker_name];\n",
        "        var domainUserId = sp.getDomainUserId();\n",
        "\n",
        "        // Calls the API\n",
        "        fetch(api_url, {\n",
        "            method: \"POST\",\n",
        "            headers: { \"Content-Type\": \"application/json\" },\n",
        "            body: JSON.stringify({ domain_userid: domainUserId })\n",
        "        })\n",
        "        .then(response => response.json())\n",
        "        .then(result => {\n",
        "            console.log(\"Prediction: \", domainUserId, \" - \", result.score);\n",
        "\n",
        "            // Acts on prediction\n",
        "            if (result.score >= 0.8) console.log('Prospect is likely to convert!');\n",
        "        })\n",
        "        .catch(console.error);\n",
        "    });\n",
        "}, 10 * 1000);\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "signals-notebooks-CrNlc6H1-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}